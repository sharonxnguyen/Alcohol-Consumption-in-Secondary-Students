---
title: 'PSTAT 131: Final Project'
author: "Sharon Nguyen, Matt Lee, Paul Song"
date: "3/8/2022"
output:
  pdf_document: default
  html_document: default
  code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(randomForest)
library(ISLR)
library(tree)
library(maptree)
library(reshape2)
library(ggplot2)
library(class)
library(FNN)
library(gridExtra)
```

# Alcohol Consumption in Secondary Students

### By Sharon Nguyen, Matt Lee, Paul Song

# Abstract

# Introduction

By finding key variables that predict final grades of students within a Portuguese secondary school, we can learn the best ways to provide support for students academically.. We want to know where students are most affected by external factors so that we can further adjust our resources to the quickly changing societal needs. 
The variables we considered were based on our own opinions about possible influences to final grades. 

For example, Drinking alcohol on weekends is not an uncommon practice in European high school student, and the assumption is this has a negative correlation to students' grades. However, we are here to find out whether this is indeed the case. 


# As a group, we've decided to go with the classification route; hence, using a decision tree, random forest, KNN, and .  We converted the type of our original dataset containing numeric variables to binary (Pass/No Pass) and ordered data. 


The variables we considered were based on our own opinions about possible influences to final grades.

# Why we chose this data

We chose this particular dataset because as college students, final grades matter the most. What we do in our freetime greatly influences our performance in school. Drinking alcohol on a weekend is not uncommon for many college students. This project is relatable in terms of what factors affect us as students. We wanted to take it upon ourselves to investigate a real world problem that we struggle with and can seek knowledge from.

Grades are heavily influenced by a lot of factors. When taking into account final grades, many factors come into play. Did the student have access to internet? How long does it take for the student to get to school? Does the student have a lot of free time on their hands? Or even, did the student spend too much time consuming alcohol? Utilizing the dataset from UCI Machine Learning, our group is attempting to see the significance that certain variables have in predicting a student's final grade.

We chose to look at the data concerning students in a Portuguese class as it has a total of 649 observations.

# Why might this model be useful?

# Loading Data and Packages

```{r}
dat <- read.csv("student-por.csv")

# Select wanted variables for analysis
data <- dat %>% select(traveltime, studytime, failures, higher, internet, famrel, freetime, goout, Dalc, Walc, health, absences, G2, G3)

str(data)
```

# Variable Analysis

These are the key variables that our final project will utilize within our modeling. A brief explanation of each is provided down below.

`traveltime`: home to school travel time (numeric: 1 - \<15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - \>1 hour)

`studytime`: weekly study time (numeric: 1 - \<2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - \>10 hours)

`failures`: number of past class failures (numeric: n if 1\<=n\<3, else 4)

`higher`: wants to take higher education (binary: yes or no)

`internet`: Internet access at home (binary: yes or no)

`famrel`: quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

`freetime`: free time after school (numeric: from 1 - very low to 5 - very high)

`goout`: going out with friends (numeric: from 1 - very low to 5 - very high)

`Dalc`: workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

`Walc`: weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

`health`: current health status (numeric: from 1 - very bad to 5 - very good)

`absences`: number of school absences (numeric: from 0 to 93)

`G3`: final grade (numeric: from 0 to 20, output target)

# Data Cleaning

```{r}
# Make variables ordered
data$traveltime <- factor(data$traveltime, ordered=TRUE, labels = c('<15 min', '15 to 30 min.', '30 min. to 1 hour', '>1 hour'))
  # ordered(data$traveltime, levels = c(1:4), labels = c('<15 min', '15 to 30 min.', '30 min. to 1 hour', '>1 hour')) 
data$studytime <- factor(data$traveltime, ordered=TRUE, labels = c('<2 hours', '2 to 5 hours', '5 to 10 hours', '>10 hours'))
  # ordered(data$studytime, levels = c(1:4), labels = c('<2 hours', '2 to 5 hours', '5 to 10 hours', '>10 hours'))
data$failures <- ordered(data$failures, levels = c(0:3), labels = c('0', '1', '2', '3'))
data$higher <- factor(data$higher, labels = c('no', 'yes'))
data$internet <- factor(data$internet, labels = c('no', 'yes'))

# famrel to health
for(i in 6:11){
  data[,i] <- ordered(data[,i], levels = c(1:5), labels = c('1', '2', '3', '4', '5')) 
}

# view strucutre
str(data)
```

```{r}
# 2 Binary Pass/No Pass
data <- data %>% mutate(grade=ifelse(G3/20 >= 0.7, 1, 0))

data <- data %>% mutate(grade_2=factor(ifelse(G3/20 >= 0.7, 'Pass', 'No Pass'),
                                       levels = c('No Pass', 'Pass')))

# # 5 Letter Grading Scale
# data <- data %>% 
#   mutate(grade_5= ifelse(G3/20 >= 18/20, 'A', G3)) %>%
#   mutate(grade_5= ifelse(G3/20 >= 16/20 &&  G3/20 <= 17/20, 'B', G3)) %>%
#   mutate(grade_5= ifelse(G3/20 >= 14/20 &&  G3/20 <= 15/20, 'C', G3)) %>%
#   mutate(grade_5= ifelse(G3/20 >= 12/20 &&  G3/20 <= 13/20, 'D', G3)) %>% 
#   mutate(grade_5= ifelse(G3/20 >= 10/20 &&  G3/20 <= 11/20, 'E', G3))

str(data)
```

# Data Split

```{r}
set.seed(123)
num_samp <- 0.7 * nrow(data)

t <- model.matrix(grade_2 ~ .-G3-grade, data) 
# 
train = sample(nrow(t), num_samp)
x.train = t[train, ]
y.train = data[train, ]$grade_2
# 
# # The rest as test data
x.test = t[-train, ]
y.test = data[-train, ]$grade_2


train.indices <- sample(nrow(data), num_samp)
train <- data[train.indices,]
test <- data[-train.indices,]
```


# Exploratory Data Analysis

```{r}
for(i in 1:14){
 print(as.data.frame(table(data[,i])))
}
```

```{r fig.height = 12, fig.width = 6, warning=FALSE}
```

# Exploratory Data Analysis

```{r fig.height = 12, fig.width = 6}
# for(i in colnames(data)){
#  print(ggplot(data, aes(x=i)) + geom_histogram(fill='lightblue', stat="count"))
# }

t <- ggplot(data, aes(x=traveltime)) + geom_histogram(fill='lightblue', stat="count")
s <- ggplot(data, aes(x=studytime)) + geom_histogram(fill='lightblue', stat="count")
hi <- ggplot(data, aes(x=higher)) + geom_histogram(fill='lightblue', stat="count")
i <- ggplot(data, aes(x=internet)) + geom_histogram(fill='lightblue', stat="count")
fa <- ggplot(data, aes(x=famrel)) + geom_histogram(fill='lightblue', stat="count")
fr <- ggplot(data, aes(x=freetime)) + geom_histogram(fill='lightblue', stat="count")
g <- ggplot(data, aes(x=goout)) + geom_histogram(fill='lightblue', stat="count")
d <- ggplot(data, aes(x=Dalc)) + geom_histogram(fill='lightblue', stat="count")
w <- ggplot(data, aes(x=Walc)) + geom_histogram(fill='lightblue', stat="count")
he <- ggplot(data, aes(x=health)) + geom_histogram(fill='lightblue', stat="count")
a <- ggplot(data, aes(x=absences)) + geom_histogram(fill='lightblue', stat="count")
g3 <- ggplot(data, aes(x=G3)) + geom_histogram(fill='lightblue', stat="count")
g <- ggplot(data, aes(x=grade)) + geom_histogram(fill='lightblue', stat="count")

grid.arrange(t,s,hi,i,fa, fr, g, d, w, he, a, g3, g, ncol=2)


# ggplot(melt(data),aes(x=value)) + geom_histogram(fill='lightblue', stat="count") + facet_wrap(~variable)
# ggplot(melt(data[,1]], id.vars=1:4),aes(x=value)) + geom_histogram() + facet_wrap(~variable)
```

```{r}
# correlation matrix
cor_train <- train

cor_train <- cor_train %>% select(absences, G2, G3)
cormat <- round(cor(cor_train), 2) 
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_raster()

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)
upper_tri

# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()
# Print the heatmap
print(ggheatmap)


ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```

```{r}

table(data$Dalc)

table(data$Walc)
```

# Decision Tree

```{r}
# Entire Data Tree with chosen variables
dat.tree <- tree(grade_2 ~ .-G3-grade, data = data)
summary(dat.tree)
draw.tree(dat.tree, nodeinfo=TRUE, cex = 0.5)
title("Classification Tree Built on All Data Set")
```

```{r}
data.tree <- tree(grade_2 ~ .-G3-grade, data = data, subset = train.indices)
summary(data.tree)

# length(which(train$grade == 0))
# plot(data.tree)
# text(data.tree, pretty = 0, cex = 0.8)
draw.tree(data.tree, nodeinfo=TRUE, cex = 0.5)
title("Classification Tree Built on Training Set")

# Predict on test set
tree.pred <- predict(data.tree, test, type="class")

# Obtain confusion matrix
error = table(tree.pred, test$grade_2)
error

# Test accuracy rate
sum(diag(error))/sum(error)

# Test error rate (Classification Error)
1-sum(diag(error))/sum(error)
```

This approach leads to correct predictions for 93% of the locations in the test set. In other words, the test error rate is 7%. This is really equivalent to:

```{r}
mean(tree.pred != test$grade_2)
```

# Pruning
### k-fold Cross-validation
```{r}
# set random see
set.seed(123)

# K-fold cross validation
cv <- cv.tree(data.tree, FUN = prune.misclass, K=10)
cv$size

# Cross-validation error
cv$dev

# Best size
# tree with 2 nodes is the lowest error
best.cv = min(cv$size[cv$dev == min(cv$dev)])
best.cv
```

## Error vs. Best Size plot
```{r}
# Plot size vs. cross-validation error rate
plot(cv$size , cv$dev, type="b",
     xlab = "Number of leaves, \'best\'", ylab = "CV Misclassification Error",
     col = "red", main="CV")
abline(v=best.cv, lty=2)
# Add lines to identify complexity parameter
min.error = which.min(cv$dev) # Get minimum error index abline(h = cv$dev[min.error],lty = 2)
```
## Prune tree
```{r}
# Prune data.tree
pt.cv = prune.misclass (data.tree, best=best.cv)

# Plot pruned tree
plot(pt.cv)
text(pt.cv, pretty=0, col = "blue", cex = .5)
draw.tree(pt.cv, nodeinfo=TRUE, cex = 0.5)
title("Pruned tree of size 2")
```

# Random Forest
```{r}
# Random Forest
data.rf <- randomForest(grade_2 ~ .-G3-grade, data = data, subset = train.indices, norm.votes = FALSE)
print(data.rf)

plot(data.rf, main='Random Forest Model')

# test error rate calculations
yhat.rf <- predict(data.rf, newdata=test)

# confusion matrix
rf.err <- table(pred = yhat.rf, truth=test$grade_2)
test.rf.err <- 1 - sum(diag(rf.err))/sum(rf.err)
test.rf.err # 0.07692308

importance(data.rf)

varImpPlot(data.rf)
varImpPlot(data.rf, sort=T, main='Predictor Importance for Random Forest Model')
```



# K Nearest Neighbors

```{r}
# KNN
set.seed(123)

pred.ytrain <- knn(train=x.train, test=x.train, cl=y.train, k=5)

conf.train <- table(predicted=pred.ytrain, true=y.train)
conf.train

# training error rate
1 - sum(diag(conf.train)/sum(conf.train))


# training classifier, making prediction on test set - KNN
pred.ytest <- knn(train=x.train, test=x.test, cl=y.train, k=5)

conf.test <- table(predicted=pred.ytest, true=y.test)
conf.test

# testing error rate
1 - sum(diag(conf.test)/sum(conf.test)) 
```
After training the classifier our predictions on the training set led to a training error rate of 7.49%. Afterwards, we trained the classifier and made predictions on the test set, resulting in a testing error rate equal to 8.71%. We then decided to increase our k and see if it would alter the training error rate and test error rate. 

```{r}
set.seed(123)

pred.ytrain <- knn(train=x.train, test=x.train, cl=y.train, k=10)

conf.train <- table(predicted=pred.ytrain, true=y.train)
conf.train

# training error rate
1 - sum(diag(conf.train)/sum(conf.train))

# training classifier, making prediction on test set - KNN
pred.ytest <- knn(train=x.train, test=x.test, cl=y.train, k=10)

conf.test <- table(predicted=pred.ytest, true=y.test)
conf.test

# testing error rate
1 - sum(diag(conf.test)/sum(conf.test)) 

```
Similar to the initial K-Nearest-Neighbors approach, after training the classifier, our predictions on the training set led to a 7.71%% training error rate and a testing error rate of 8.71%. Our training error rate increased by .22% and our testing error rate remained the same. 

We see that our testing error rate is higher then our training error rate. 



# Analysis of the Test Set

# Conclusion

# Reference

[<https://www.studyineurope.eu/study-in-portugal/grades>](https://www.studyineurope.eu/study-in-portugal/grades){.uri}

## An Overview of Your Dataset

The data was obtained in a survey of students' math course in secondary school in Portugal. Called "Student Alcohol Consumption", the dataset can be found on kaggle at <https://www.kaggle.com/uciml/student-alcohol-consumption?select=student-mat.csv>. It includes social, gender, and study information about students.

In the dataset, we will be dealing with both qualitative and quantitative variables. It contains a total of 395 observations with 33 predictors, containing no missing data.

## An Overview of Your Research Question

We are interested in predicting the number of times a student consumes alcohol every week. We want to figure out whether variables such as free time and parent's education status has an effect on the number of times a student drinks alcohol each week. Our response variable will be Dalc (Workday alcohol consumption) and Walc (Weekend alcohol consumption) added together to make Alc (Weekly alcohol consumption).

We believe that the question would be best answered with a classification approach. Of these 33 predictors, we find that final grade, free time, absences, family's education support, study time, parent cohabitation status, parent's education, number of failed classes, quality of family relationship, and going out with friends will be useful for our prediction.

# Comments

Predict G1, G2, and G3
