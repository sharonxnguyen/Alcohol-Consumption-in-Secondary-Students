---
title: 'PSTAT 131: Final Project'
author: "Sharon Nguyen, Matt Lee, Paul Song"
date: "3/8/2022"
output:
  html_document:
      toc: true
      theme: united
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(randomForest)
library(ISLR)
library(tree)
library(maptree)
library(reshape2)
library(ggplot2)
library(class)
library(FNN)
library(gridExtra)
library(grid)
library(gt)
```

# Alcohol Consumption in Secondary Students

### By Sharon Nguyen, Matt Lee, Paul Song

# Abstract

# Introduction

By finding key variables that predict final grades of students within a Portuguese secondary school, we can learn the best ways to provide support for students academically.. We want to know where students are most affected by external factors so that we can further adjust our resources to the quickly changing societal needs. The variables we considered were based on our own opinions about possible influences to final grades.

For example, Drinking alcohol on weekends is not an uncommon practice amongst European high school students, and the assumption is this has a negative correlation to students' grades. However, we are here to find out whether this is indeed the case.

In our project, we decided to take a dataset containing numerous characteristics of students in a secondary (high) school in Porto, Portugal in an attempt to predict the final grade of students based on these attributes. We mixed and matched different characteristics to find significant or intriguing correlation between such characteristics.

As a group, we've decided to go with the classification route; hence, using a decision tree, random forest, and KNN. We converted the type of our original dataset containing numeric variables to binary (Pass/No Pass) and ordered data.

# Why we chose this data

We chose this particular dataset because as college students, final grades matter the most. What we do in our freetime greatly influences our performance in school. Drinking alcohol on a weekend is not uncommon for many college students. This project is relatable in terms of what factors affect us as students. We wanted to take it upon ourselves to investigate a real world problem that we struggle with and can seek knowledge from.

Grades are heavily influenced by a lot of factors. When taking into account final grades, many factors come into play. Did the student have access to internet? How long does it take for the student to get to school? Does the student have a lot of free time on their hands? Or even, did the student spend too much time consuming alcohol? Utilizing the dataset from UCI Machine Learning, our group is attempting to see the significance that certain variables have in predicting a student's final grade.

We chose to look at the data concerning students in a Portuguese class as it has a total of 649 observations.

# 

# Loading Data and Packages

```{r}
dat <- read.csv("student-por.csv")

# Select wanted variables for analysis
data <- dat %>% select(traveltime, studytime, failures, higher, internet, famrel, freetime, goout, Dalc, Walc, health, absences, G2, G3)

str(data)
```

# Variable Analysis

These are the key variables that our final project will utilize within our modeling. A brief explanation of each is provided down below.

`traveltime`: home to school travel time (numeric: 1 - \<15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - \>1 hour)

`studytime`: weekly study time (numeric: 1 - \<2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - \>10 hours)

`failures`: number of past class failures (numeric: n if 1\<=n\<3, else 4)

`higher`: wants to take higher education (binary: yes or no)

`internet`: Internet access at home (binary: yes or no)

`famrel`: quality of family relationships (numeric: from 1 - very bad to 5 - excellent)

`freetime`: free time after school (numeric: from 1 - very low to 5 - very high)

`goout`: going out with friends (numeric: from 1 - very low to 5 - very high)

`Dalc`: workday alcohol consumption (numeric: from 1 - very low to 5 - very high)

`Walc`: weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)

`health`: current health status (numeric: from 1 - very bad to 5 - very good)

`absences`: number of school absences (numeric: from 0 to 93)

`G2`: second period grade (numeric: from 0 to 20)

`G3`: final grade (numeric: from 0 to 20, output target)

# Data Cleaning

```{r}
# Make variables ordered
data$traveltime <- factor(data$traveltime, ordered=TRUE, labels = c('<15 min', '15 to 30 min.', '30 min. to 1 hour', '>1 hour'))
  # ordered(data$traveltime, levels = c(1:4), labels = c('<15 min', '15 to 30 min.', '30 min. to 1 hour', '>1 hour')) 
data$studytime <- factor(data$traveltime, ordered=TRUE, labels = c('<2 hours', '2 to 5 hours', '5 to 10 hours', '>10 hours'))
  # ordered(data$studytime, levels = c(1:4), labels = c('<2 hours', '2 to 5 hours', '5 to 10 hours', '>10 hours'))
data$failures <- ordered(data$failures, levels = c(0:3), labels = c('0', '1', '2', '3'))
data$higher <- factor(data$higher, labels = c('no', 'yes'))
data$internet <- factor(data$internet, labels = c('no', 'yes'))
data$famrel <- factor(data$famrel, ordered=TRUE, labels = c("very bad", "bad", "fair", "good", "excellent"))

# freetime to Walc
for(i in 7:10){
  data[,i] <- factor(data[,i], ordered=TRUE, labels = c("very low", "low", "medium", "high", "very high"))
}

data$health <- factor(data$health, ordered=TRUE, labels = c("very bad", "bad", "fair", "good", "very good"))
# data$G2 <- ordered(data$G2, levels = c(0:20))
# data$G3 <- ordered(data$G3, levels = c(0:20))

# 2 Binary Pass/No Pass
#data <- data %>% mutate(grade=ifelse(G3/20 >= 0.7, 1, 0))

data <- data %>% mutate(grade=factor(ifelse(G3/20 >= 0.7, 'Pass', 'No Pass'),
                                       levels = c('No Pass', 'Pass')))

str(data)

# view strucutre
str(data)
```

$$\text{grade} = \left\{
\begin{array}{ll}
      \text{No Pass}, & \text{if } [\frac{\text{G3}}{20} \times 100] < 70\%  \\
      \text{Pass}, & \text{if } [\frac{\text{G3}}{20} \times 100] \ge 70\% \\ 
\end{array} 
\right. 
$$

# Data Split

We split the data by indexing for grade, the binary value after taking into account G2 and G3 results.

```{r}
set.seed(123)
num_samp <- 0.7 * nrow(data)

t <- model.matrix(grade ~ .-G3, data)

# 
train = sample(nrow(t), num_samp)
x.train = t[train, ]
y.train = data[train, ]$grade
# 
# # The rest as test data
x.test = t[-train, ]
y.test = data[-train, ]$grade


train.indices <- sample(nrow(data), num_samp)
train <- data[train.indices,]
test <- data[-train.indices,]
```

# Exploratory Data Analysis

```{r}
for(i in 1:14){
 print(table(data[,i]))
}
```

```{r fig.height = 12, fig.width = 6, warning=FALSE}
plotlist <- list()
for(i in 1:15){
 p <- ggplot(data, aes_string(x=data[,i])) + geom_histogram(fill='lightblue', stat="count") + xlab(colnames(data)[i])
 plotlist[[i]] <- p
}
grid.arrange(grobs=plotlist, ncol=2, top = "Histograms of All Data Variables")
```

```{r}
# correlation matrix
cor_train <- train

cor_train <- cor_train %>% select(absences, G3, G2)
cormat <- round(cor(cor_train), 2) 
melted_cormat <- melt(cormat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_raster()

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)
upper_tri

# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}

# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()
# # Print the heatmap
# print(ggheatmap)


ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```

From the heat map, we can see absences and final grade (G3) have low correlation. However, we can also see that second period grade (G2) and final grades (G3) have very positive correlation.

# Decision Tree

```{r}
# Entire Data Tree with chosen variables
dat.tree <- tree(grade~ .-G3, data = data)
summary(dat.tree)
draw.tree(dat.tree, nodeinfo=TRUE, cex = 0.5)
title("Classification Tree Built on All Data Set")
```

```{r}
data.tree <- tree(grade~ .-G3, data = data, subset = train.indices)
summary(data.tree)

# length(which(train$grade == 0))
# plot(data.tree)
# text(data.tree, pretty = 0, cex = 0.8)
draw.tree(data.tree, nodeinfo=TRUE, cex = 0.5)
title("Classification Tree Built on Training Set")

# Predict on test set
tree.pred <- predict(data.tree, test, type="class")

# Obtain confusion matrix
error = table(tree.pred, test$grade)
error

# Test accuracy rate
sum(diag(error))/sum(error)

# Test error rate (Classification Error)
1-sum(diag(error))/sum(error)
```

This approach leads to correct predictions for 93% of the locations in the test set. In other words, the test error rate is 7%. This is really equivalent to:

```{r}
mean(tree.pred != test$grade)
```

# Pruning

### k-fold Cross-validation

```{r}
# set random see
set.seed(123)

# K-fold cross validation
cv <- cv.tree(data.tree, FUN = prune.misclass, K=10)
cv$size

# Cross-validation error
cv$dev

# Best size
# tree with 2 nodes is the lowest error
best.cv = min(cv$size[cv$dev == min(cv$dev)])
best.cv
```

## Error vs. Best Size plot

```{r}
# Plot size vs. cross-validation error rate
plot(cv$size , cv$dev, type="b",
     xlab = "Number of leaves, \'best\'", ylab = "CV Misclassification Error",
     col = "red", main="CV")
abline(v=best.cv, lty=2)
# Add lines to identify complexity parameter
min.error = which.min(cv$dev) # Get minimum error index 
abline(h = cv$dev[min.error],lty = 2)
```

## Prune tree

```{r}
# Prune data.tree
pt.cv = prune.misclass (data.tree, best=best.cv)

# Plot pruned tree
draw.tree(pt.cv, nodeinfo=TRUE, cex = 0.5)
title("Pruned tree of size 2")
```

## Respective Test Error Rate for model pt.cv

```{r}
# Predict on test set
pred.pt.cv = predict(pt.cv, test, type="class") # Obtain confusion matrix
err.pt.cv = table(pred.pt.cv, test$grade)
err.pt.cv

 # Test accuracy rate
sum(diag(err.pt.cv))/sum(err.pt.cv)

# Test error rate (Classification Error)
1-sum(diag(err.pt.cv))/sum(err.pt.cv)
```

The test error rate for pt.cv is 7%, which is identical to the test error rate for when we used function `tree()`. This means we get a simpler tree for free (without any cost in prediction error rate) by pruning. Thus, we prefer the pruned tree.

# Random Forest

```{r}
# Random Forest
data.rf <- randomForest(grade ~ .-G3, data = data, subset = train.indices, norm.votes = FALSE)
print(data.rf)

plot(data.rf, main='Random Forest Model')

# test error rate calculations
yhat.rf <- predict(data.rf, newdata=test)

# confusion matrix
rf.err <- table(pred = yhat.rf, truth=test$grade)
test.rf.err <- 1 - sum(diag(rf.err))/sum(rf.err)
test.rf.err # 0.07692308

importance(data.rf)

varImpPlot(data.rf)
varImpPlot(data.rf, sort=T, main='Predictor Importance for Random Forest Model')
```

# K Nearest Neighbors

```{r}
# KNN
set.seed(123)

pred.ytrain <- knn(train=x.train, test=x.train, cl=y.train, k=5)

conf.train <- table(predicted=pred.ytrain, true=y.train)
conf.train

# training error rate
1 - sum(diag(conf.train)/sum(conf.train))


# training classifier, making prediction on test set - KNN
pred.ytest <- knn(train=x.train, test=x.test, cl=y.train, k=5)

conf.test <- table(predicted=pred.ytest, true=y.test)
conf.test

# testing error rate
1 - sum(diag(conf.test)/sum(conf.test)) 
```

After training the classifier our predictions on the training set led to a training error rate of 7.49%. Afterwards, we trained the classifier and made predictions on the test set, resulting in a testing error rate equal to 8.71%. We then decided to increase our k and see if it would alter the training error rate and test error rate.

```{r}
set.seed(123)

pred.ytrain <- knn(train=x.train, test=x.train, cl=y.train, k=10)

conf.train <- table(predicted=pred.ytrain, true=y.train)
conf.train

# training error rate
1 - sum(diag(conf.train)/sum(conf.train))

# training classifier, making prediction on test set - KNN
pred.ytest <- knn(train=x.train, test=x.test, cl=y.train, k=10)

conf.test <- table(predicted=pred.ytest, true=y.test)
conf.test

# testing error rate
1 - sum(diag(conf.test)/sum(conf.test)) 

```

Similar to the initial K-Nearest-Neighbors approach, after training the classifier, our predictions on the training set led to a 7.71%% training error rate and a testing error rate of 8.71%. Our training error rate increased by .22% and our testing error rate remained the same.

We see that our testing error rate is higher then our training error rate.

# Analysis of the Test Set

# Conclusion

In conclusion, after running different classification algorithms to predict `grade`, we were only able to accurately use G2 as a predictor for final grades. 

# not useful in predicting final grade,  

# References

[\<https://www.studyineurope.eu/study-in-portugal/grades>](https://www.studyineurope.eu/study-in-portugal/grades){.uri}

[\<https://www.kaggle.com/uciml/student-alcohol-consumption?select=student-mat.csv>](https://www.kaggle.com/uciml/student-alcohol-consumption?select=student-mat.csv){.uri}.
